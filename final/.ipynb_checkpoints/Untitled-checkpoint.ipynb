{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class to calculate several simialrity metrics for an input list of peptides given a sequence string to compare it to.\n",
    "USAGE: \n",
    "1. Create SequenceSimilarityObject({sequence string}, {dictionary of data paths} (to be deprecated soon -- use\n",
    "   internal class data), {path to peptides csv}, {column title of sequence values of aforementioned peptides csv})\n",
    "   for any number of sequences you want to be compared to the list of peptides\n",
    "2. For each object, call object.generate_similarity() to fill out the Dataframe with similarity metrics\n",
    "3. To whittle down this similarity matrix to list only those peptides with pattern matching of a minimum length\n",
    "   at a matching index in the rerence peptide (henceforth the binder), call object.get_df_with_binder_subseqs(min_length={#})\n",
    "@ Author: Chris Pecunies, with help from Savvy Gupta and Aaron Tsang\n",
    "@ Date: February 12, 2020\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Set, Tuple, Dict, List\n",
    "from scipy import interpolate, stats, fftpack, signal\n",
    "#import sci-kit learn\n",
    "import textdistance as td\n",
    "\n",
    "class SequenceSimilarity:\n",
    "    '''\n",
    "    Class that takes in a path to a list of amino acid sequences as well\n",
    "    as any number of peptide sequences explicitly that are known to have\n",
    "    a certain set of properties. Generates metrics for similarity for each\n",
    "    peptide in path and returns domains AA sequence with high similarity\n",
    "    '''\n",
    "    AA = list('FYWAVILMSTNQPGDEKHRC')\n",
    "    \n",
    "    def __init__(self, binder: Tuple,\n",
    "                 data: SeqData\n",
    "                 peps_path: str,       \n",
    "                 aa_col: str,\n",
    "                 min_length: int = 0,\n",
    "                 dists: List = [],\n",
    "                 only_match: bool = False):       \n",
    "        \n",
    "        # ---- setting data ---------\n",
    "        \n",
    "        #self.conv = [\"NUM\", \"EIIP\", \"FNS\"]\n",
    "        self.d = {d: vars(data)[d] for d in list(vars(data).keys())}\n",
    "        self.conv = list(sl['conv'].keys())\n",
    "        \n",
    "            \n",
    "        self.aa_col = [aa_col]\n",
    "        self.sim_cols = ['PAM30', 'BLOSUM', 'RRM_SN', 'RRM_Corr', 'weighted_matches']\n",
    "        self.match_cols = ['sseq_matches', ]\n",
    "        self.conv_cols = [conv+'_Seq' for conv in self.conv]\n",
    "        self.cols = self.aa_col + self.sim_cols + self.conv_cols + self.match_cols\n",
    "        \n",
    "        self.d['conv'] = dict.fromkeys(self.conv)\n",
    "        self.d['conv']['NUM'] = [0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 5, 5, 6, 6, 6, 7]\n",
    "        self.d['conv']['EIIP'] = [0.0946, 0.0516, 0.0548, 0.0373, 0.0057, 0.0, 0.0, 0.0823,\\\n",
    "                     0.0829, 0.0941, 0.0036, 0.0761, 0.0198, 0.005, 0.1263, 0.0058, \\\n",
    "                     0.0371, 0.0242, 0.0959, 0.0829]\n",
    "        self.d['conv']['FNS'] = ['Aromatic', 'Aromatic', 'Aromatic', 'Hydrophobic', 'Hydrophobic', \\\n",
    "                    'Hydrophobic', 'Hydrophobic', 'Hydrophobic', 'Polar', 'Polar', 'Polar', \\\n",
    "                    'Polar', 'Proline', 'Glycine', 'Charge (-)', 'Charge (-)', 'Charge (+)', \\\n",
    "                    'Charge (+)', 'Charge (+)', 'Excluded']\n",
    "        self.AA_map = {cnv:dict(zip(self.AA, self.d['conv'][cnv])) for cnv in self.conv}\n",
    "        self.bname, self.binder = binder\n",
    "        self.bsseq = [(binder[i:j], i) for i, _ in enumerate(binder) for j in range(i+1, len(binder)+1)]\n",
    "        self.__read_similarity_data(data_paths) # !TO BE DEPRECATED! Just store data in class\n",
    "        \n",
    "        # ---- helper lambda functions\n",
    "        self.AA_conv = lambda typ, pep: tuple(self.AA_map[typ][AA] if AA in self.AA else 0 for AA in pep)\n",
    "        self.get_sim = lambda p1, p2, t: sum([self.data[t][a1][a2] for a1 in p1 for a2 in p2])\n",
    "        \n",
    "        self.peps_og = pd.read_csv(peps_path)\n",
    "        self.peps_og.columns = self.aa_col\n",
    "        self.peps_og = self.peps_og.drop_duplicates()\n",
    "        self.peps_og = self.peps_og[~self.peps_og[aa_col].str.contains(\"O\")]\n",
    "        self.peps = self.peps_og[self.peps_og[aa_col].str.len() == len(binder)]\n",
    "        self.peps_match = self.pdata_match[aa_col]\n",
    "        if not self.peps: \n",
    "            raise Exception(\"No peptides of same length as binder found\")\n",
    "        if only_match: self.peps = self.pmatch\n",
    "        self.pdata = pd.Dataframe(index=self.peps.index, column=self.aa_col + self.cols)\n",
    "        self.pdata_match = pd.concat([self.df_filter_subseq(ss,i) for (ss,i) in self.bsseq if len(ss) >= min_len])\n",
    "        self.update_similarities(dists)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #----------------SET UP FUNCTIONS (void)-------------------------------#\n",
    "            \n",
    "    def _update_AA_conversion(self) -> None:\n",
    "        \"\"\"if seq.signaltonoise(cross) < max(sn):\n",
    "        Adds to the initially empty column values for conv_type (possible choices\n",
    "        'EIIP' and 'Num' for now) the conversion of the AA sequence in the self.aa_col\n",
    "        column a list representing its conversion\n",
    "        \"\"\"\n",
    "        for conv in enumerate(list(self.d['conv'].keys())):\n",
    "            if conv == 'NUM':\n",
    "                self.pdata[conv+\"_Seq\"] = [str([str(n) for n in self.AA_conv(conv, p)]) for p in self.peps]\n",
    "            else:\n",
    "                self.pdata[conv+\"_Seq\"] = [self.AA_conv(conv, p) for p in self.peps]\n",
    "    \n",
    "    def _update_matrix_similarity(self) -> None:\n",
    "        \"\"\"\n",
    "        Just updates the similarity columns for the output similarity dataframe.\n",
    "        Uses lambda helper function self.get_sim in __init__\n",
    "        \"\"\"\n",
    "        for m in list(self.d['matr'].keys()):\n",
    "            sim = [self.get_sim(p, self.binder, m) for p in self.peps]\n",
    "            self.pdata[m] = np.interp(sim, (min(sim), max(sim)), (0,1))\n",
    "        \n",
    "\n",
    "    def _update_RRM_similarity(self) -> None:\n",
    "        \"\"\"\n",
    "        Uses the Resonant Recognition Model as described by Irena Cosic to \n",
    "        \"\"\"\n",
    "        \n",
    "        bnd_eiip = self.AA_conv('EIIP', self.binder)\n",
    "        bnd_dft = get_dft_from_eiip(bnd_eiip)\n",
    "        sn = []\n",
    "        do = []\n",
    "        best_sn = None\n",
    "        best_dot = None\n",
    "        \n",
    "        def signaltonoise(a, axis=0, ddof=0):\n",
    "            a = np.asanyarray(a)\n",
    "            m = a.mean(axis)\n",
    "            sd = a.std(axis=axis, ddof=ddof)\n",
    "            return np.where(sd == 0, 0, m/sd)\n",
    "        \n",
    "        for pep in self.peps:\n",
    "            seq_eiip =self.AA_conv('EIIP', pep)\n",
    "            seq_dft = np.fft.rfft(seq_eiip)\n",
    "            \n",
    "            cross = signal.coherence(seq_dft, bnd_dft)\n",
    "            dot = np.dot(seq_dft, bnd_dft)\n",
    "            SN = np.mean(np.real(signaltonoise(cross, axis=None)))\n",
    "            do.append(dot)\n",
    "            sn.append(SN)\n",
    "            if not sn and SN >= max(sn):\n",
    "                best_sn = (pep, seq_dft)\n",
    "            if not do or dot >= max(do):\n",
    "                best_dot = (pep, seq_dft)\n",
    "            \n",
    "        sn_out = np.interp(sn, (min(sn), max(sn)), (0,1))\n",
    "        dot_out = np.interp(do, (min(do), max(do)), (0,1))\n",
    "        self.pdata['RRM_Corr'] = dot_out\n",
    "        self.pdata['RRM_SN'] = sn_out\n",
    "        \n",
    "    def get_spectrums() -> pd.DataFrame:\n",
    "        pass\n",
    "    \n",
    "    # NOTE! Adds columns \"Matching_sseqs\" and \"Num_matching\" to output\n",
    "    # Might be too unwieldy / unhelpful for output similarity data\n",
    "    # if so, just comment out _update_matching_sseqs()\n",
    "    def _update_matching_sseqs(self, single_match_weight: float = 1, weight: float = 1) -> None:\n",
    "        \"\"\"\n",
    "        Returns a number as a new column representing the number of \"matches\" a peptide\n",
    "        has for all possible subsequences for the binder inputted at a given index. For\n",
    "        weighting=1, all matches are treated equally ('Y' at position 3 is treated equal\n",
    "        to IMV at position 0) but lowering weighting lowers smaller-length matches\n",
    "        \"\"\"\n",
    "        # @TODO Remove \"duplicates\" which occur at different matching indexes of binder\n",
    "        # but are part of a larger pattern already recorded at an earlier index\n",
    "        self.pdata['sseq_matches'] = None\n",
    "        self.pdata['weighted_matches'] = None\n",
    "        \n",
    "        score: float = lambda s: (single_match_weight * 1) + (len(s)**weight)\n",
    "        matches: List = list(); nmatches = list()\n",
    "        for i, seq in enumerate(self.peps):\n",
    "            matches.append(list()); nmatches.append(int())\n",
    "            for j, AA in enumerate(seq): \n",
    "                lmatch: Tuple = None\n",
    "                for k, (sseq, bin_i) in enumerate(self.bsseq):\n",
    "                    in_seq = seq[j:len(sseq)+j]\n",
    "                    if (bin_i == j) and (in_seq == sseq): \n",
    "                        if lmatch is not None:\n",
    "                            if seq[lmatch[0]:lmatch[0]+lmatch[1]].find(in_seq) >= 0:\n",
    "                                continue\n",
    "                        if self.bsseq[k-1][1] == bin_i:\n",
    "                            nmatches[i] -= score(matches[i].pop()[0])\n",
    "                            lmatch = (bin_i, len(sseq))\n",
    "                        matches[i].append((sseq, bin_i))\n",
    "                        nmatches[i] += score(sseq)\n",
    "        \n",
    "        self.pdata['sseq_matches'] = matches\n",
    "        self.pdata['weighted_matches'] = np.interp(nmatches, (min(nmatches), max(nmatches)), (0,1))\n",
    "        self.sim_cols += ['weighted_matches']\n",
    "        self.cols += ['sseq_matches', 'weighted_matches']\n",
    "        \n",
    "                    \n",
    "    def _remove_matching_sseqs_column(self) -> None:\n",
    "        \"\"\"\n",
    "        Just removes the binder sseq pattern matches list column and number\n",
    "        of matching (with/without) weighting if they exist\n",
    "        \"\"\"\n",
    "        if self.pdata.columns.contains(self.match_cols):\n",
    "            self.cols.remove(self.match_cols)\n",
    "            self.pdata = self.pdata.drop(columns=self.match_cols)\n",
    "            \n",
    "    def _update_distances(self, metrics: List = []) -> None:\n",
    "        \"\"\"\n",
    "        Adds Hamming, Levenstein, etc. distance metrics for sequences in peptide list\n",
    "        Metrics can be specified by name string in parameter\n",
    "        \"\"\"\n",
    "        dists = metrics if metrics else list(self.d['dist'].keys())\n",
    "        dist_vals = [[d['dist'][d](p, self.binder) for p in self.peps] for d in dists]\n",
    "        self.pdata[dists] = dist_vals\n",
    "        self.cols += dists\n",
    "        self.sim_cols += dists\n",
    "                \n",
    "                \n",
    "    def update_similarities(self, use_distance=False, metrics: List = []) -> None:\n",
    "        '''\n",
    "        Updates the similarity values whenever called (for now should be only once right\n",
    "        after creating the object, ecept possibly if the Binding peptide is updated\n",
    "        (should be handled automatically)\n",
    "        '''\n",
    "        self._update_AA_conversion()\n",
    "        self._update_matrix_similarity()\n",
    "        self._update_RRM_similarity()\n",
    "        self._update_matching_sseqs()\n",
    "        if use_distance: \n",
    "            self._update_distances(metrics) if metrics else self._update_distances()\n",
    "        # OPTIONAL\n",
    "        # self._unpack_num_encoding()\n",
    "        \n",
    "    #----------MAIN CLASS FUNCTIONS (returns data) ------------------------#\n",
    "\n",
    "    def df_filter_subseq(self, sub_seq: str, ind: int = None) -> pd.DataFrame:\n",
    "        '''\n",
    "        Takes in a subsequence of equal or lesser length to\n",
    "        peptides in class peptide dataframe and returns a dataframe\n",
    "        containing only those peptides containing the sequence\n",
    "        '''\n",
    "        if not {*sub_seq}.issubset({*self.AA}):\n",
    "            raise Exception('Invalid subsequence')\n",
    "        if ind is None:\n",
    "            return self.pdata[self.peps.str.contains(sub_seq)]\n",
    "        return self.pdata[self.peps.str.find(sub_seq) == ind]\n",
    "\n",
    "    def get_sim_matrix(self, seq) -> pd.DataFrame:\n",
    "        return self.data.filter\n",
    "        \n",
    "    \n",
    "    def merge_data(self, other, sep_cols = False) -> pd.DataFrame:\n",
    "        # !!! IMPORTANT: \"other\" must also be SequenceSimilarity object (couldnt compile)\n",
    "        \"\"\"\n",
    "        Returns a merged Dataframe of self.pdata and another SequenceSimilarity's pep_data.\n",
    "        If sep_cols=True, then the other SequenceSimilarity's columns will simply be appended\n",
    "        to the returned DataFrame (self.pdata is unchanged). If False, results will be averaged.\n",
    "        @NOTE: This is a super naive implementatoin -- expand this to make it more configurable\n",
    "        @TODO: Take in *others as a list of arbitrarily many other SequenceSimilarities to compare\n",
    "        \"\"\"\n",
    "        # must be same length binders -> so same peptides of interest\n",
    "        this_data = self.pdata.copy()\n",
    "        non_seq_cols = self.sim_cols.copy()\n",
    "        seq_cols = [self.aa_col] + self.conv_cols\n",
    "        other_data = other.pep_data.copy()\n",
    "        if sep_cols:\n",
    "            if len(list(this_data.columns)) == len(list(other_data.columns)):\n",
    "                d = this_data.merge(right=other_data, on=self.aa_col, suffixes=(\"_\"+self.bname, \"_\"+other.bname))\n",
    "                return d.drop_duplicates()\n",
    "            else:\n",
    "                raise Exception(\"Mismatched columns\")\n",
    "\n",
    "        new_cols = ['{}_{}_{}'.format(col, self.bname, other.bname) for col in self.sim_cols]\n",
    "        out_data = pd.DataFrame(index=this_data.index, columns=seq_cols + new_cols)\n",
    "        out_data[seq_cols] = this_data[seq_cols]\n",
    "        both = pd.concat([this_data[non_seq_cols],other_data[non_seq_cols]])\n",
    "        out_data[new_cols] = both.groupby(both.index).mean()\n",
    "        if 'sseq_matches' in self.cols or 'sseq_matches' in other.columns:\n",
    "            both_match = self.pdata['sseq_matches'].append(other.pep_data['sseq_matches'])\n",
    "            both['sseq_matches'] = both_match\n",
    "            out_data.join(both_match)\n",
    "        return out_data\n",
    "                    \n",
    "        #@TODO Finish\n",
    "                \n",
    "                \n",
    "    #-------------------miscellaneous methods------------------------------#\n",
    "    \n",
    "#     def get_kendalltau_corr_map(self) -> Tuple:\n",
    "#         return stats.kendalltau(self.data['AA_MAP'][['Num']], self.data['AA_MAP'][['EIIP']])\n",
    "    \n",
    "class SeqData:\n",
    "\n",
    "    CONV = {\n",
    "        'NUM': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 5, 5, 6, 6, 6, 7],\n",
    "        'EIIP' : [0.0946, 0.0516, 0.0548, 0.0373, 0.0057, 0.0, 0.0, 0.0823,\\\n",
    "                 0.0829, 0.0941, 0.0036, 0.0761, 0.0198, 0.005, 0.1263, 0.0058, \\\n",
    "                 0.0371, 0.0242, 0.0959, 0.0829],\n",
    "        'FNS' : ['Aromatic', 'Aromatic', 'Aromatic', 'Hydrophobic', 'Hydrophobic', \\\n",
    "                'Hydrophobic', 'Hydrophobic', 'Hydrophobic', 'Polar', 'Polar', 'Polar', \\\n",
    "                'Polar', 'Proline', 'Glycine', 'Charge (-)', 'Charge (-)', 'Charge (+)', \\\n",
    "                'Charge (+)', 'Charge (+)', 'Excluded'],\n",
    "    }\n",
    "    # @TODO: Import Biopython dicts of matrices, not read from .csvs\n",
    "    MATR = {\n",
    "        'PAM30': pd.read_csv('./src_data/pam30.csv', index_col=0).to_dict(),\n",
    "        'BLOSUM45': pd.read_csv('./src_data/BLOSUM.csv', index_col=0).to_dict(),\n",
    "    }\n",
    "    DIST = {\n",
    "        'jaro_winkler': (lambda p1, p2: td.jaro_winkler.normalized_similarity(p1, p2)),\n",
    "        'needleman_wunsch': (lambda p1, p2: td.needleman_wunsch.normalized_similarity(p1, p2)),\n",
    "        'smith_waterman': (lambda p1, p2: td.smith_waterman.normalized_similarity(p1, p2)),\n",
    "        'levenshtein': (lambda p1, p2: td.levenshtein.normalized_similarity(p1, p2))\n",
    "    }\n",
    "    \n",
    "    def __init__(self, conv=CONV.keys(), matr=MATR.keys(), dist=DIST.keys()):\n",
    "        \n",
    "        self.conv = {cnv:self.CONV[cnv] for cnv in self.CONV.keys() if cnv in conv}\n",
    "        self.matr = {mtr:self.MATR[mtr] for mtr in self.MATR.keys() if mtr in matr}\n",
    "        self.dist = {dst:self.DIST[dst] for dst in self.DIST.keys() if dst in dist}\n",
    "    \n",
    "'''\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "\n",
    "\n",
    "def similarity_func(u, v):\n",
    "    return 1/(1+euclidean(u,v))\n",
    "\n",
    "DF_var = pd.DataFrame.from_dict({\"s1\":[1.2,3.4,10.2],\"s2\":[1.4,3.1,10.7],\"s3\":[2.1,3.7,11.3],\"s4\":[1.5,3.2,10.9]})\n",
    "DF_var.index = [\"g1\",\"g2\",\"g3\"]\n",
    "\n",
    "dists = pdist(DF_var, similarity_func)\n",
    "DF_euclid = pd.DataFrame(squareform(dists), columns=DF_var.index, index=DF_var.index)\n",
    "'''\n",
    "\n",
    "# @TODO Dynamically filter peptide set based on length(s) of input sequences of binders\n",
    "#       i.e. 2 binders, one 11 AA long, one 13 AA long, each gets their own \"subset\" of the\n",
    "#       full peptide lilst that can be compared to it. For any number of input sequences\n",
    "\n",
    "# @TODO implement method for both similarities to M6 and GrBP5 to interrelate and act as their own feature set:\n",
    "# i.e. if a peptide matches both peptides in temrs of sequence at some index, that should be important rather than\n",
    "# having it equal to one matching only one\n",
    "\n",
    "# @TODO: Maybe as originally planned implement binders inputted as dictionary of multiple values, each with separate\n",
    "#      dataframes within the same class --> for big similarity calculations. Or just create multiple SequenceSimilarity instances\n",
    "# @TODO: Implement method to apply relevant similarity scoring algorithms (distance metrics, ex) for\n",
    "#       non-same-length peptides. Could even be stored alongside same length peptides\n",
    "# @TODO Simplify some of the df filtering going on with Dataframe.where() or Dataframe.mask or Dataframe.query or isin\n",
    "# @TODO Implement some patterns as being DEFINING features of binders -- ie. endi with letter 1 letter 2 letter 2 letter 1 or starting with IMVT always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance as td\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NUM', 'EIIP', 'FNS']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = SeqData()\n",
    "sl = {d: vars(data)[d] for d in list(vars(data).keys())}\n",
    "list(sl['conv'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
